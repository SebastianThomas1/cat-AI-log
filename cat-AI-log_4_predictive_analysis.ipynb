{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cat-AI-log. An AI-based product group allocation system\n",
    "\n",
    "Capstone project.\n",
    "\n",
    "Sebastian Thomas @ neue fische Bootcamp Data Science<br />\n",
    "(datascience at sebastianthomas dot de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Predictive analysis\n",
    "\n",
    "We vectorize our text data and fit some predictive models.\n",
    "\n",
    "## Imports\n",
    "\n",
    "### Modules, classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of modules\n",
    "from importlib import import_module\n",
    "\n",
    "# python object persistence\n",
    "import joblib\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MaxAbsScaler, Normalizer\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# custom modules\n",
    "from modules.ds_rough import Vectorizer, Scaler\n",
    "from modules.quotient_extraction import QuotientCountVectorizer, QuotientTfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "\n",
    "Some helping functions for exploration and a list for estimator selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarities_and_certainties(classifier, corpus, y,\n",
    "                                      normal_vectorizer=make_pipeline(CountVectorizer(), Normalizer()),\n",
    "                                      test_size=0.1):\n",
    "    corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, y, test_size=test_size, stratify=y,\n",
    "                                                                  random_state=0)\n",
    "\n",
    "    ndt = normal_vectorizer.fit_transform(corpus_train)\n",
    "\n",
    "    classifier.fit(corpus_train, y_train);\n",
    "\n",
    "    corpus_test_correct = corpus_test[y_test == classifier.predict(corpus_test)]\n",
    "    corpus_test_incorrect = corpus_test[y_test != classifier.predict(corpus_test)]\n",
    "\n",
    "    bins = np.linspace(0., 1., 6)\n",
    "\n",
    "    similarities_correct = np.max(ndt.dot(normal_vectorizer.transform(corpus_test_correct).transpose()),\n",
    "                                  axis=0).toarray().flatten()\n",
    "    similarities_correct_binned = pd.cut(np.round(similarities_correct, 2), bins, include_lowest=True)\n",
    "\n",
    "    similarities_incorrect = np.max(ndt.dot(normal_vectorizer.transform(corpus_test_incorrect).transpose()),\n",
    "                                    axis=0).toarray().flatten()\n",
    "    similarities_incorrect_binned = pd.cut(np.round(similarities_incorrect, 2), bins, include_lowest=True)\n",
    "\n",
    "    certainties_correct = np.max(classifier.predict_proba(corpus_test_correct), axis=1)\n",
    "    certainties_correct_binned = pd.cut(np.round(certainties_correct, 2), bins, include_lowest=True)\n",
    "\n",
    "    certainties_incorrect = np.max(classifier.predict_proba(corpus_test_incorrect), axis=1)\n",
    "    certainties_incorrect_binned = pd.cut(np.round(certainties_incorrect, 2), bins, include_lowest=True)\n",
    "\n",
    "    print('mean similarity of correctly classified:   {:.3f}'.format(np.mean(similarities_correct)))\n",
    "    print('mean similarity of incorrectly classified: {:.3f}'.format(np.mean(similarities_incorrect)))\n",
    "    print('mean certainty of correctly classified:    {:.3f}'.format(np.mean(certainties_correct)))\n",
    "    print('mean certainty of incorrectly classified:  {:.3f}'.format(np.mean(certainties_incorrect)))\n",
    "\n",
    "    (fig, ax) = plt.subplots(2, 2, figsize=(13.5, 9), dpi=300)\n",
    "\n",
    "    sns.countplot(similarities_correct_binned, ax=ax[0, 0])\n",
    "    sns.countplot(similarities_incorrect_binned, ax=ax[0, 1])\n",
    "    sns.countplot(certainties_correct_binned, ax=ax[1, 0])\n",
    "    sns.countplot(certainties_incorrect_binned, ax=ax[1, 1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_selection = [\n",
    "    ('sklearn.naive_bayes', 'BernoulliNB', 'bernoulli_naive_bayes_classifier',\n",
    "     {\n",
    "         'alpha':     [1.0e-10, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "         'fit_prior': [True, False]\n",
    "     }),\n",
    "    ('sklearn.naive_bayes', 'MultinomialNB', 'multinomial_naive_bayes_classifier',\n",
    "     {\n",
    "         'alpha':     [0.0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "         'fit_prior': [True, False]\n",
    "     }),\n",
    "    ('sklearn.svm', 'SVC', 'linear_support_vector_classifier',\n",
    "     {\n",
    "         'kernel':       ['linear'],\n",
    "         'C':            [10**r for r in range(-5, 1)],\n",
    "         'shrinking':    [True, False],\n",
    "         'random_state': [0]\n",
    "     }),\n",
    "    ('sklearn.svm', 'SVC', 'poly_support_vector_classifier',\n",
    "     {\n",
    "         'kernel':       ['poly'],\n",
    "         'degree':       list(range(2, 4)),\n",
    "         'C':            [10**r for r in range(-5, 1)],\n",
    "         'gamma':        [10**r for r in range(-5, 1)],\n",
    "         'shrinking':    [True, False],\n",
    "         'random_state': [0]\n",
    "     }),\n",
    "    ('sklearn.svm', 'SVC', 'sigmoid_support_vector_classifier',\n",
    "     {\n",
    "         'kernel':       ['sigmoid'],\n",
    "         'C':            [10**r for r in range(-5, 6)],\n",
    "         'gamma':        [10**r for r in range(-5, 4)],\n",
    "         'shrinking':    [True, False],\n",
    "         'random_state': [0]\n",
    "     }),\n",
    "    ('sklearn.svm', 'SVC', 'rbf_support_vector_classifier',\n",
    "     {\n",
    "         'kernel':       ['rbf'],\n",
    "         'C':            [10**r for r in range(-5, 4)],\n",
    "         'gamma':        [10**r for r in range(-5, 4)],\n",
    "         'shrinking':    [True, False],\n",
    "         'random_state': [0]\n",
    "     }),\n",
    "    ('sklearn.ensemble', 'RandomForestClassifier', 'random_forest_classifier',\n",
    "     {\n",
    "         'criterion':         ['gini', 'entropy'],\n",
    "         'n_estimators':      [300, 400, 500, 600, 1000],\n",
    "         'max_depth':         list(range(5, 30, 2)), # with None, we get an overfit!\n",
    "         'min_samples_split': list(range(2, 11)),\n",
    "         'min_samples_leaf':  list(range(1, 11)),\n",
    "         'random_state':      [0]\n",
    "     }),\n",
    "#    ('sklearn.linear_model', 'LogisticRegression', 'logistic_discrimination_classifier',\n",
    "#     {\n",
    "#         'C':             [10**r for r in range(-5, 4)],\n",
    "#         'fit_intercept': [True, False],\n",
    "#         'penalty':       ['l1', 'l2'],\n",
    "#         'random_state':  [0]\n",
    "#     }),\n",
    "#    ('sklearn.naive_bayes', 'GaussianNB', 'gaussian_naive_bayes_classifier',\n",
    "#     {\n",
    "#         'var_smoothing': [10**r for r in range(-15, 2)]\n",
    "#     }),\n",
    "#    ('sklearn.neighbors', 'KNeighborsClassifier', 'k_nearest_neighbors_classifier',\n",
    "#     {\n",
    "#         'n_neighbors': list(range(1, 21)),\n",
    "#         'algorithm':   ['ball_tree', 'kd_tree', 'brute'],\n",
    "#         'leaf_size':   list(range(15, 55, 5)),\n",
    "#         'p':           list(range(1, 6))\n",
    "#     }),\n",
    "#    ('sklearn.tree', 'DecisionTreeClassifier', 'decision_tree_classifier',\n",
    "#     {\n",
    "#         'criterion':         ['gini', 'entropy'],\n",
    "#         'splitter':          ['best', 'random'],\n",
    "#         'max_depth':         list(range(5, 21, 2)) + [None],\n",
    "#         'min_samples_split': list(range(2, 11)),\n",
    "#         'min_samples_leaf':  list(range(1, 22, 3)),\n",
    "#         'max_features':      [r*0.1 for r in range(1, 11)] + ['auto', 'sqrt', 'log2', None],\n",
    "#         'random_state':      [0]\n",
    "#     }),\n",
    "#    ('xgboost', 'XGBClassifier', 'xgb_classifier',\n",
    "#     {\n",
    "#         'learning_rate':     [0.05, 0.10, 0.20, 0.30],\n",
    "#         'n_estimators':      [100, 150, 200, 250],\n",
    "#         'max_depth':         [5, 6, 7, 8, 9, 10],\n",
    "#         'min_child_weight':  [1, 3, 5],\n",
    "#         'gamma':             [0.0, 0.1, 0.2, 0.3],\n",
    "#         'colsample_bytree':  [0.4, 0.5, 0.6, 0.7],\n",
    "#         'reg_lambda':        [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "#    })\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We import our cleaned data with engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira = pd.read_pickle('data/mira_2.pickle')\n",
    "mira.sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the data set that we use to construct the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we will make changes to this data set, we need to copy it\n",
    "mira_clfd = mira[mira['product group'].notna()].copy()\n",
    "mira_clfd[['article', 'article base', 'product group']].sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the four least frequent occuring product groups have very few instances, we change the value of the target `'product group'` of these instances to `'Varia'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_frequent_product_groups = mira_clfd.groupby(['product group'])['article']\\\n",
    "                                .count().sort_values(ascending=False).index[-4:]\n",
    "\n",
    "def reduce_to_most_frequent(product_group):\n",
    "    return 'Varia' if product_group in least_frequent_product_groups else product_group\n",
    "\n",
    "labels = mira_clfd['product group'].apply(reduce_to_most_frequent)\n",
    "labels.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There remain 13 labels. We encode these labels as non-negative integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach with `'article'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first approach, we work with the unprocessed feature `'article'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = mira_clfd['article'].values\n",
    "\n",
    "corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, y, test_size=0.1, stratify=y,\n",
    "                                                              random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore some facts about the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train instances:   {:<5}'.format(corpus_train.shape[0]))\n",
    "print('test instances:    {:<5}'.format(corpus_test.shape[0]))\n",
    "print('tokens in X_train: {:<5}'.format(CountVectorizer().fit_transform(corpus_train).shape[1]))\n",
    "print('tokens in X:       {:<5}'.format(CountVectorizer().fit_transform(corpus).shape[1]))\n",
    "print('all tokens:        {:<5}'.format(CountVectorizer().fit_transform(mira['article']).shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a rough analysis with some vectorizers and scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fit, corpus_valid, y_fit, y_valid = train_test_split(corpus_train, y_train, test_size=0.1,\n",
    "                                                            stratify=y_train, random_state=0)\n",
    "\n",
    "for vec_type in ['count', 'tfidf']:\n",
    "    for sc_type in [None, 'standard', 'maxabs', 'norm']:\n",
    "        clf = make_pipeline(Vectorizer(vec_type), Scaler(sc_type), MultinomialNB())\n",
    "        clf.fit(corpus_fit, y_fit)\n",
    "        print('{:<5} {:<8} {:<3} {:.3f}'.format(vec_type, str(sc_type), 'NB', clf.score(corpus_valid, y_valid)))\n",
    "\n",
    "        clf = make_pipeline(Vectorizer(vec_type), Scaler(sc_type), SVC(random_state=0))\n",
    "        clf.fit(corpus_fit, y_fit)\n",
    "        print('{:<5} {:<8} {:<3} {:.3f}'.format(vec_type, str(sc_type), 'SVC', clf.score(corpus_valid, y_valid)))\n",
    "\n",
    "        clf = make_pipeline(Vectorizer(vec_type), Scaler(sc_type), RandomForestClassifier(random_state=0))\n",
    "        clf.fit(corpus_fit, y_fit)\n",
    "        print('{:<5} {:<8} {:<3} {:.3f}'.format(vec_type, str(sc_type), 'RF', clf.score(corpus_valid, y_valid)))\n",
    "\n",
    "        clf = make_pipeline(Vectorizer(vec_type), Scaler(sc_type), XGBClassifier(random_state=0))\n",
    "        clf.fit(corpus_fit, y_fit)\n",
    "        print('{:<5} {:<8} {:<3} {:.3f}'.format(vec_type, str(sc_type), 'XGB', clf.score(corpus_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a systematic randomized search with various machine learning algorithms (NaÃ¯ve Bayes, Support Vector Classifier, Random Forest) and save the best results to a pandas dataframe. As the computation takes a couple of minutes, we persist the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    classifier_comparison = pd.read_pickle('results/based_on_article.pickle')\n",
    "except FileNotFoundError:\n",
    "    classifier_comparison = pd.DataFrame(columns=['Module', 'Class', 'Name', 'BestParameters', 'BestScore'])\n",
    "\n",
    "    for (module_name, class_name, classifier_name, parameters) in classifier_selection:\n",
    "        print('Randomized search for {}.{:<80}'.format(module_name, class_name), end='\\r')\n",
    "        module_of_classifier = import_module(module_name)\n",
    "        class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "        classifier = Pipeline([('vct', 'passthrough'),\n",
    "                               ('scl', 'passthrough'),\n",
    "                               (classifier_name, class_of_classifier())])\n",
    "        parameters = {classifier_name + '__' + param: parameters[param]\n",
    "                      for param in parameters.keys()}\n",
    "        parameters.update({\n",
    "            'vct': [CountVectorizer(), TfidfVectorizer()],\n",
    "            'scl': ['passthrough', StandardScaler(with_mean=False), MaxAbsScaler(), Normalizer()],        \n",
    "        })\n",
    "        rs = RandomizedSearchCV(estimator=classifier, n_iter=200, param_distributions=parameters,\n",
    "                                scoring='accuracy', cv=10, return_train_score=True, verbose=0, n_jobs=-1,\n",
    "                                random_state=0)\n",
    "        rs.fit(corpus_train, y_train)\n",
    "\n",
    "        classifier_comparison = classifier_comparison.append({'Module': module_name,\n",
    "                                                              'Class': class_name,\n",
    "                                                              'Name': classifier_name,\n",
    "                                                              'BestParameters': rs.best_params_,\n",
    "                                                              'BestScore': rs.best_score_},\n",
    "                                                             ignore_index=True)\n",
    "    \n",
    "    classifier_comparison.to_pickle(path='results/based_on_article.pickle')\n",
    "    classifier_comparison.to_csv(path_or_buf='results/based_on_article.csv', index=False)\n",
    "\n",
    "classifier_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best found classifier is a support vector classifier with rbf kernel. It has a mean accoury on the validation sets of roughly 73.0%.\n",
    "\n",
    "We analyse the similarities and certainties of the best found classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_estimator(idx):\n",
    "    module_name = classifier_comparison.loc[idx, 'Module']\n",
    "    class_name = classifier_comparison.loc[idx, 'Class']\n",
    "    classifier_name = classifier_comparison.loc[idx, 'Name']\n",
    "    parameters = classifier_comparison.loc[idx, 'BestParameters']\n",
    "    if classifier_name.endswith('support_vector_classifier'):\n",
    "        parameters.update({\n",
    "                classifier_name + '__probability': True\n",
    "            })\n",
    "\n",
    "    module_of_classifier = import_module(module_name)\n",
    "    class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "    classifier = Pipeline([('vct', parameters['vct']),\n",
    "                           ('scl', parameters['scl']),\n",
    "                           (classifier_name, class_of_classifier())])\n",
    "    classifier.set_params(**parameters)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "plot_similarities_and_certainties(load_estimator(5), corpus_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean similarities and the mean certainties of the correctly and incorrectly classified validation instances do not differ much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach with `'article base'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we work with the processed feature `'article base'`. The steps are the same as in the first approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = mira_clfd['article base'].values\n",
    "\n",
    "corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, y, test_size=0.1, stratify=y,\n",
    "                                                              random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore some facts about the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train instances:   {:<5}'.format(corpus_train.shape[0]))\n",
    "print('test instances:    {:<5}'.format(corpus_test.shape[0]))\n",
    "print('tokens in X_train: {:<5}'.format(CountVectorizer().fit_transform(corpus_train).shape[1]))\n",
    "print('tokens in X:       {:<5}'.format(CountVectorizer().fit_transform(corpus).shape[1]))\n",
    "print('all tokens:        {:<5}'.format(CountVectorizer().fit_transform(mira['article base']).shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    classifier_comparison = pd.read_pickle('results/based_on_article_base.pickle')\n",
    "except FileNotFoundError:\n",
    "    classifier_comparison = pd.DataFrame(columns=['Module', 'Class', 'Name', 'BestParameters', 'BestScore'])\n",
    "\n",
    "    for (module_name, class_name, classifier_name, parameters) in classifier_selection:\n",
    "        print('Randomized search for {}.{:<80}'.format(module_name, class_name), end='\\r')\n",
    "        module_of_classifier = import_module(module_name)\n",
    "        class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "        classifier = Pipeline([('vct', 'passthrough'),\n",
    "                               ('scl', 'passthrough'),\n",
    "                               (classifier_name, class_of_classifier())])\n",
    "        parameters = {classifier_name + '__' + param: parameters[param]\n",
    "                      for param in parameters.keys()}\n",
    "        parameters.update({\n",
    "            'vct': [CountVectorizer(), TfidfVectorizer()],\n",
    "            'scl': ['passthrough', StandardScaler(with_mean=False), MaxAbsScaler(), Normalizer()],        \n",
    "        })\n",
    "        rs = RandomizedSearchCV(estimator=classifier, n_iter=200, param_distributions=parameters,\n",
    "                                scoring='accuracy', cv=10, return_train_score=True, verbose=0, n_jobs=-1,\n",
    "                                random_state=0)\n",
    "        rs.fit(corpus_train, y_train)\n",
    "\n",
    "        classifier_comparison = classifier_comparison.append({'Module': module_name,\n",
    "                                                              'Class': class_name,\n",
    "                                                              'Name': classifier_name,\n",
    "                                                              'BestParameters': rs.best_params_,\n",
    "                                                              'BestScore': rs.best_score_},\n",
    "                                                             ignore_index=True)\n",
    "    \n",
    "    classifier_comparison.to_pickle(path='results/based_on_article_base.pickle')\n",
    "    classifier_comparison.to_csv(path_or_buf='results/based_on_article_base.csv', index=False)\n",
    "\n",
    "classifier_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarities_and_certainties(load_estimator(5), corpus_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recognize that there is a large gap between the (mean) similarities resp. (mean) certainties of correctly and incorrectly classified validation instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach with `'article base'` and `'dosage form'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we work with the features `'article base'` and `'dosage form'`. This leads to a slightly more complicated pipeline since we now have to use two vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = mira_clfd[['article base', 'dosage form']].fillna('').values\n",
    "\n",
    "corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, y, test_size=0.1, stratify=y,\n",
    "                                                              random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    classifier_comparison = pd.read_pickle('results/based_on_article_base_and_dosage_form.pickle')\n",
    "except FileNotFoundError:\n",
    "    classifier_comparison = pd.DataFrame(columns=['Module', 'Class', 'Name', 'BestParameters', 'BestScore'])\n",
    "\n",
    "    for (module_name, class_name, classifier_name, parameters) in classifier_selection:\n",
    "        print('Randomized search for {}.{:<80}'.format(module_name, class_name), end='\\r')\n",
    "        module_of_classifier = import_module(module_name)\n",
    "        class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "        classifier = Pipeline([('ct', ColumnTransformer([('vct1', 'passthrough', 0),\n",
    "                                                         ('vct2', 'passthrough', 1)])),\n",
    "                               ('scl', 'passthrough'),\n",
    "                               (classifier_name, class_of_classifier())])\n",
    "        parameters = {classifier_name + '__' + param: parameters[param]\n",
    "                      for param in parameters.keys()}\n",
    "        parameters.update({\n",
    "            'ct__vct1': [CountVectorizer(), TfidfVectorizer()],\n",
    "            'ct__vct2': [CountVectorizer(), TfidfVectorizer()],\n",
    "            'scl': ['passthrough', StandardScaler(with_mean=False), MaxAbsScaler(), Normalizer()],        \n",
    "        })\n",
    "        rs = RandomizedSearchCV(estimator=classifier, n_iter=200, param_distributions=parameters,\n",
    "                                scoring='accuracy', cv=10, return_train_score=True, verbose=0, n_jobs=-1,\n",
    "                                random_state=0)\n",
    "        rs.fit(corpus_train, y_train)\n",
    "\n",
    "        classifier_comparison = classifier_comparison.append({'Module': module_name,\n",
    "                                                              'Class': class_name,\n",
    "                                                              'Name': classifier_name,\n",
    "                                                              'BestParameters': rs.best_params_,\n",
    "                                                              'BestScore': rs.best_score_},\n",
    "                                                             ignore_index=True)\n",
    "    \n",
    "    classifier_comparison.to_pickle(path='results/based_on_article_base_and_dosage_form.pickle')\n",
    "    classifier_comparison.to_csv(path_or_buf='results/based_on_article_base_and_dosage_form.csv', index=False)\n",
    "\n",
    "classifier_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_estimator(idx):\n",
    "    module_name = classifier_comparison.loc[idx, 'Module']\n",
    "    class_name = classifier_comparison.loc[idx, 'Class']\n",
    "    classifier_name = classifier_comparison.loc[idx, 'Name']\n",
    "    parameters = classifier_comparison.loc[idx, 'BestParameters'].copy()\n",
    "    if classifier_name.endswith('support_vector_classifier'):\n",
    "        parameters.update({\n",
    "                classifier_name + '__probability': True\n",
    "            })\n",
    "\n",
    "    module_of_classifier = import_module(module_name)\n",
    "    class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "    classifier = Pipeline([('ct', ColumnTransformer([('vct1', 'passthrough', 0),\n",
    "                                                     ('vct2', 'passthrough', 1)])),\n",
    "                           ('scl', 'passthrough'),\n",
    "                           (classifier_name, class_of_classifier())])\n",
    "    classifier.set_params(**parameters)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "normal_vectorizer = make_pipeline(ColumnTransformer([('vct1', CountVectorizer(), 0),\n",
    "                                                     ('vct2', CountVectorizer(), 1)]), Normalizer())\n",
    "plot_similarities_and_certainties(load_estimator(4), corpus_train, y_train, normal_vectorizer=normal_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the (mean) similarities resp. certainties gets smaller again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach with `'article base'` and extracted features\n",
    "\n",
    "Next, we also work with the occurance of some numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['mass concentration 0 [mg/ml]', 'mass puff concentration 0 [mg/hub]', 'mass flow 0 [mg/h]',\n",
    "                      'volume flow 0 [ml/h]', 'active ingredient percentage 0 [%]', 'mass 0 [mg]',\n",
    "                      'volume 0 [ml]', 'count puff 0 [hub]', 'percentage 0 [%]', 'length 0 [cm]', 'count 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.concatenate([mira_clfd[['article base', 'dosage form']].fillna(''),\n",
    "                         mira_clfd[numerical_features].notna()], axis=1)\n",
    "\n",
    "corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, y, test_size=0.1, stratify=y,\n",
    "                                                              random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    classifier_comparison = pd.read_pickle('results/based_on_article_base_and_extracted_features.pickle')\n",
    "except FileNotFoundError:\n",
    "    classifier_comparison = pd.DataFrame(columns=['Module', 'Class', 'Name', 'BestParameters', 'BestScore'])\n",
    "\n",
    "    for (module_name, class_name, classifier_name, parameters) in classifier_selection:\n",
    "        print('Randomized search for {}.{:<80}'.format(module_name, class_name), end='\\r')\n",
    "        module_of_classifier = import_module(module_name)\n",
    "        class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "        classifier = Pipeline([('ct', ColumnTransformer([('vct1', 'passthrough', 0),\n",
    "                                                         ('vct2', 'passthrough', 1),\n",
    "                                                         ('pt', 'passthrough', slice(2, 16))])),\n",
    "                               ('scl', 'passthrough'),\n",
    "                               (classifier_name, class_of_classifier())])\n",
    "        parameters = {classifier_name + '__' + param: parameters[param]\n",
    "                      for param in parameters.keys()}\n",
    "        parameters.update({\n",
    "            'ct__vct1': [CountVectorizer(), TfidfVectorizer()],\n",
    "            'ct__vct2': [CountVectorizer(), TfidfVectorizer()],\n",
    "            'scl': ['passthrough', StandardScaler(with_mean=False), MaxAbsScaler(), Normalizer()],        \n",
    "        })\n",
    "        rs = RandomizedSearchCV(estimator=classifier, n_iter=200, param_distributions=parameters,\n",
    "                                scoring='accuracy', cv=10, return_train_score=True, verbose=0, n_jobs=-1,\n",
    "                                random_state=0)\n",
    "        rs.fit(corpus_train, y_train)\n",
    "\n",
    "        classifier_comparison = classifier_comparison.append({'Module': module_name,\n",
    "                                                              'Class': class_name,\n",
    "                                                              'Name': classifier_name,\n",
    "                                                              'BestParameters': rs.best_params_,\n",
    "                                                              'BestScore': rs.best_score_},\n",
    "                                                             ignore_index=True)\n",
    "    \n",
    "    classifier_comparison.to_pickle(path='results/based_on_article_base_and_extracted_features.pickle')\n",
    "    classifier_comparison.to_csv(path_or_buf='results/based_on_article_base_and_extracted_features.csv', index=False)\n",
    "\n",
    "classifier_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_estimator(idx):\n",
    "    module_name = classifier_comparison.loc[idx, 'Module']\n",
    "    class_name = classifier_comparison.loc[idx, 'Class']\n",
    "    classifier_name = classifier_comparison.loc[idx, 'Name']\n",
    "    parameters = classifier_comparison.loc[idx, 'BestParameters'].copy()\n",
    "    if classifier_name.endswith('support_vector_classifier'):\n",
    "        parameters.update({\n",
    "                classifier_name + '__probability': True\n",
    "            })\n",
    "\n",
    "    module_of_classifier = import_module(module_name)\n",
    "    class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "    classifier = Pipeline([('ct', ColumnTransformer([('vct1', 'passthrough', 0),\n",
    "                                                     ('vct2', 'passthrough', 1),\n",
    "                                                     ('pt', 'passthrough', slice(2, 16))])),\n",
    "                           ('scl', 'passthrough'),\n",
    "                           (classifier_name, class_of_classifier())])\n",
    "    classifier.set_params(**parameters)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "normal_vectorizer = make_pipeline(ColumnTransformer([('vct1', CountVectorizer(), 0),\n",
    "                                                     ('vct2', CountVectorizer(), 1),\n",
    "                                                     ('pt', 'passthrough', slice(2, 16))]), Normalizer())\n",
    "plot_similarities_and_certainties(load_estimator(1), corpus_train, y_train, normal_vectorizer=normal_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the differences get even smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach with `'article base'` and quotient vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we would like to try an approach based on the feature `'article base'` and quotient vectorizers. This has to be implemented in a more clever way since the fitting time of the quotient vectorizers is to high. (We could work with a precomputed vocabulary and a predefined quotient matrix, which could be sliced in the cross validation process.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = mira_clfd['article base'].values\n",
    "\n",
    "#corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, y, test_size=0.1, stratify=y,\n",
    "#                                                              random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try:\n",
    "#    classifier_comparison = pd.read_pickle('results/based_on_article_base_and_quotient_vectorizer.pickle')\n",
    "#except FileNotFoundError:\n",
    "#    classifier_comparison = pd.DataFrame(columns=['Module', 'Class', 'Name', 'BestParameters', 'BestScore'])\n",
    "\n",
    "#    for (module_name, class_name, classifier_name, parameters) in classifier_selection:\n",
    "#        print('Randomized search for {}.{:<80}'.format(module_name, class_name), end='\\r')\n",
    "#        module_of_classifier = import_module(module_name)\n",
    "#        class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "#        classifier = Pipeline([('vct', 'passthrough'),\n",
    "#                               ('scl', 'passthrough'),\n",
    "#                               (classifier_name, class_of_classifier())])\n",
    "#        parameters = {classifier_name + '__' + param: parameters[param]\n",
    "#                      for param in parameters.keys()}\n",
    "#        parameters.update({\n",
    "#            'vct': [QuotientCountVectorizer(), QuotientTfidfVectorizer()],\n",
    "#            'scl': ['passthrough', StandardScaler(with_mean=False), MaxAbsScaler(), Normalizer()],        \n",
    "#        })\n",
    "#        rs = RandomizedSearchCV(estimator=classifier, n_iter=200, param_distributions=parameters,\n",
    "#                                scoring='accuracy', cv=10, return_train_score=True, verbose=0, n_jobs=-1,\n",
    "#                                random_state=0)\n",
    "#        rs.fit(corpus_train, y_train)\n",
    "\n",
    "#        classifier_comparison = classifier_comparison.append({'Module': module_name,\n",
    "#                                                              'Class': class_name,\n",
    "#                                                              'Name': classifier_name,\n",
    "#                                                              'BestParameters': rs.best_params_,\n",
    "#                                                              'BestScore': rs.best_score_},\n",
    "#                                                             ignore_index=True)\n",
    "    \n",
    "#    classifier_comparison.to_pickle(path='results/based_on_article_base_and_quotient_vectorizer.pickle')\n",
    "#    classifier_comparison.to_csv(path_or_buf='results/based_on_article_base_and_quotient_vectorizer.csv', index=False)\n",
    "\n",
    "#classifier_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_estimator(idx):\n",
    "#    module_name = classifier_comparison.loc[idx, 'Module']\n",
    "#    class_name = classifier_comparison.loc[idx, 'Class']\n",
    "#    classifier_name = classifier_comparison.loc[idx, 'Name']\n",
    "#    parameters = classifier_comparison.loc[idx, 'BestParameters']\n",
    "#    if classifier_name.endswith('support_vector_classifier'):\n",
    "#        parameters.update({\n",
    "#                classifier_name + '__probability': True\n",
    "#            })\n",
    "#\n",
    "#    module_of_classifier = import_module(module_name)\n",
    "#    class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "#    classifier = Pipeline([('tfidf', parameters['tfidf']),\n",
    "#                           ('scl', parameters['scl']),\n",
    "#                           (classifier_name, class_of_classifier())])\n",
    "#    classifier.set_params(**parameters)\n",
    "    \n",
    "#    return classifier\n",
    "\n",
    "#plot_similarities_and_certainties(load_estimator(5), corpus_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier based on `'article base'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we work with the feature `'article base'` again and study the results of a voting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = mira_clfd['article base'].values\n",
    "\n",
    "corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, y, test_size=0.1, stratify=y,\n",
    "                                                              random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_comparison = pd.read_pickle('results/based_on_article_base.pickle')\n",
    "classifier_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_estimator(idx):\n",
    "    module_name = classifier_comparison.loc[idx, 'Module']\n",
    "    class_name = classifier_comparison.loc[idx, 'Class']\n",
    "    classifier_name = classifier_comparison.loc[idx, 'Name']\n",
    "    parameters = classifier_comparison.loc[idx, 'BestParameters']\n",
    "    if classifier_name.endswith('support_vector_classifier'):\n",
    "        parameters.update({\n",
    "                classifier_name + '__probability': True\n",
    "            })\n",
    "\n",
    "    module_of_classifier = import_module(module_name)\n",
    "    class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "    classifier = Pipeline([('vct', parameters['vct']),\n",
    "                           ('scl', parameters['scl']),\n",
    "                           (classifier_name, class_of_classifier())])\n",
    "    classifier.set_params(**parameters)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "for voting in ['hard', 'soft']:\n",
    "    classifier = VotingClassifier([\n",
    "        ('clf1', load_estimator(5)),\n",
    "        ('clf2', load_estimator(2)),\n",
    "        ('clf3', load_estimator(4)),\n",
    "        ('clf4', load_estimator(1)),\n",
    "        ('clf5', load_estimator(0))\n",
    "    ],\n",
    "        voting=voting, n_jobs=-1)\n",
    "    print('{} mean valid score: {:.3f}'.format(voting,\n",
    "                                               cross_validate(classifier, corpus_train, y_train,\n",
    "                                                              cv=10, n_jobs=-1)['test_score'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 best: hard 67.4 soft 67.1\n",
    "# 4 best: hard 67.4 soft 67.2\n",
    "# 3 best: hard 67.0 soft 67.0\n",
    "# 2 best: hard 67.4 soft 66.7\n",
    "# best svc and naive bayes: hard 67.0, soft 67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarities_and_certainties(classifier, corpus_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the voting classifier does not seem to achieve better results, we select the best performing classifier on the feature `'article base'`, train it on the whole train set and evaluate its quality on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = mira_clfd['article base'].values\n",
    "\n",
    "corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, y, test_size=0.1, stratify=y,\n",
    "                                                              random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_comparison = pd.read_pickle('results/based_on_article_base.pickle')\n",
    "classifier_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_estimator(idx):\n",
    "    module_name = classifier_comparison.loc[idx, 'Module']\n",
    "    class_name = classifier_comparison.loc[idx, 'Class']\n",
    "    classifier_name = classifier_comparison.loc[idx, 'Name']\n",
    "    parameters = classifier_comparison.loc[idx, 'BestParameters']\n",
    "    if classifier_name.endswith('support_vector_classifier'):\n",
    "        parameters.update({\n",
    "                classifier_name + '__probability': True\n",
    "            })\n",
    "\n",
    "    module_of_classifier = import_module(module_name)\n",
    "    class_of_classifier = getattr(module_of_classifier, class_name)\n",
    "    classifier = Pipeline([('vct', parameters['vct']),\n",
    "                           ('scl', parameters['scl']),\n",
    "                           (classifier_name, class_of_classifier())])\n",
    "    classifier.set_params(**parameters)\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = load_estimator(5)\n",
    "classifier.fit(corpus_train, y_train);\n",
    "print('score train: {:.3f}'.format(classifier.score(corpus_train, y_train)))\n",
    "print('score test:  {:.3f}'.format(classifier.score(corpus_test, y_test)))\n",
    "print('score all:   {:.3f}'.format(classifier.score(corpus, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarities_and_certainties(classifier, corpus_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save this development classifier for the purpose of later visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(classifier, 'objects/dev_classifier.joblib');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we retrain the selected classifier on the whole corpus for the prediction on all (in particular still unclassified) instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(corpus, y);\n",
    "print('score all:   {:.3f}'.format(classifier.score(corpus, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append the prediction and its certainty to the dataframe as well as some strings for later representation, e.g. in the web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira['prediction'] = label_encoder.inverse_transform(classifier.predict(mira['article base']))\n",
    "mira['certainty'] = np.max(classifier.predict_proba(mira['article base']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(instance):\n",
    "    product_group = instance['product group']\n",
    "    prediction = instance['prediction']\n",
    "    return prediction if pd.isna(product_group) else product_group\n",
    "def print_certainty(instance):\n",
    "    product_group = instance['product group']\n",
    "    certainty = instance['certainty']\n",
    "    if pd.isna(product_group):\n",
    "        if certainty >= 0.8:\n",
    "            return 'very certain'\n",
    "        elif certainty >= 0.5:\n",
    "            return 'certain'\n",
    "        elif certainty >= 0.2:\n",
    "            return 'uncertain'\n",
    "        elif certainty >= 0:\n",
    "            return 'very uncertain'\n",
    "    else:\n",
    "        return 'confirmed'\n",
    "    #return str(round(certainty * 100)) if pd.isna(product_group) else 'confirmed'\n",
    "\n",
    "mira['prediction print'] = mira.apply(print_prediction, axis=1)\n",
    "mira['certainty print'] = mira.apply(print_certainty, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the performance of the classifier by investigation of some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira[['article', 'article base', 'product group', 'prediction', 'certainty', 'prediction print',\n",
    "      'certainty print']][mira['article'].str.contains(r'\\bAspirin\\b', flags=re.IGNORECASE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira[['article', 'article base', 'product group', 'prediction', 'certainty', 'prediction print',\n",
    "      'certainty print']][mira['article'].str.contains(r'\\bSymbicort\\b', flags=re.IGNORECASE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira[['article', 'article base', 'product group', 'prediction', 'certainty', 'prediction print',\n",
    "      'certainty print']][mira['article'].str.contains(r'\\bIbuprofen\\b', flags=re.IGNORECASE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira[['article', 'article base', 'product group', 'prediction', 'certainty', 'prediction print',\n",
    "      'certainty print']][mira['article'].str.contains(r'\\bHydrocortison\\b', flags=re.IGNORECASE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also illustrate that the classifier does not predict anything useful on instances it had no chance to learn something about. As there are much more tokens in the whole data set compared to the preclassified data set, this result is not surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira[['article', 'article base', 'product group', 'prediction', 'certainty', 'prediction print',\n",
    "      'certainty print']][mira['article'].str.contains(r'\\bHÃ¼ft\\b', flags=re.IGNORECASE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira[['article', 'article base', 'product group', 'prediction', 'certainty', 'prediction print',\n",
    "      'certainty print']][mira['article'].str.contains(r'\\bHÃ¼ft\\b', flags=re.IGNORECASE)\n",
    "                          & mira['product group'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data set\n",
    "\n",
    "We save the extended data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira.to_pickle('data/mira_processed.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save classifier and label encoder\n",
    "\n",
    "We save the classifier as well as the label encoder for later usage, e.g. in the web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(classifier, 'objects/classifier.joblib');\n",
    "joblib.dump(label_encoder, 'objects/label_encoder.joblib');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/corpus_train.npy', corpus_train)\n",
    "np.save('data/corpus_test.npy', corpus_test)\n",
    "np.save('data/y_train.npy', y_train)\n",
    "np.save('data/y_test.npy', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
